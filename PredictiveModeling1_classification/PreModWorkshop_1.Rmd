---
title: "Predictive Model Workshop"
author: "Yuxiao Luo"
date: "3/18/2021"
output:
  md_document:
    variant: markdown_github
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Build a logistic regression model

### Titanic dataset description
The `titanic` library in R has 4 different datasets, they are **titanic_train**, **titanic_test**, **titanic_gender_model**, and **titanic_gender_class_model**. They describe the survival status of individual passengers on the Titanic. The variable description can be found below:  

1. Pclass: Passenger Class (1 = 1st, 2 = 2nd, 3 = 3rd)  

2. survived: Survival (0 = No; 1 = Yes) 

3. name: Name  

4. sex: Sex  

5. age: Age  

6. sibsp: Number of Siblings/Spouses Aboard  

7. parch: Number of Parents/Children Aboard  

8. ticket: Ticket Number  

9. fare: Passenger Fare (British pound)  

10. cabin: Cabin  

11. embarked: Port of Embarkation

    (C = Cherbourg; Q = Queenstown; S = Southampton) 
   
12. boat: Lifeboat

13. body: Body Identification Number  

14. home.dest: Home/Destination  

### Data preprocessing
We use the `titanic` library to load the data.
```{r load data, message=F, warning=F}
library(titanic)
library(tidyverse)
data("titanic_train")
data("titanic_test")
str(titanic_train)
```

Alternatively, you can input the data by downloading it from my Github repo and reading it into R with library `readr`.

```{r, warning = F}
library(readr)
titanic_train <- read_csv("data/titanic_train.csv")
titanic_test <- read_csv("data/titanic_test.csv")
```

Let's clean the dataset by extracting four variables (`Survived`, `Pclass`, `Age`,`Sex`) and change the variable type to `factor` for `Survived` and `Pclass`. Since the data has been pre-processed by other people and is pretty clean, we don't do much thing here. Usually, you will have more work to do for your raw data.

```{r, message = F, warning = F}
data("titanic_train")
data("titanic_test")

# clean the data
titanic_train <- titanic_train %>% 
  select(Survived, Pclass, Age, Sex) %>% 
  mutate(Survived = as.factor(Survived), Pclass = as.factor(Pclass)) %>%  na.omit()

titanic_test <- titanic_test %>% 
  select(Pclass, Age, Sex) %>% 
  mutate(Pclass = as.factor(Pclass)) %>% na.omit()
```


Let's do preliminary descriptive analysis using `ggpairs` function in `GGally` library. The function will generate a matrix of selected variables. 

```{r descriptive, echo=F, message=FALSE}
GGally::ggpairs(titanic_train, axisLabels = "internal")
```

### Modeling
Then, let's fit a logistic regression model to it and use `summary` function to see the model details. 

```{r}
logistic <- glm(Survived ~ Pclass + Age + Sex, family = "binomial", data = titanic_train)
summary(logistic)
```

The model looks good and all selected variables are significant, we want to explore and see whether there are better models. Let's try interaction terms. 

#### Interaction terms

An interaction occurs when an independent variable has a different effect on the outcome depending on the values of another independent variable. We want to check `Age` and `Pclass` first because typically older may earn more money and thus, buy expensive tickets. 

```{r}
mod_inte <- glm(Survived ~ Pclass + Age + Sex + Age*Pclass, family = "binomial", data = titanic_train)
summary(mod_inte)
```

The result doesn't show any trace. Then, let's do a full interaction model and try every possible combination. 

```{r}
mod_inte <- glm(Survived ~ Pclass*Age*Sex, family = "binomial", data = titanic_train)
summary(mod_inte)
```

It looks good though. Why don't we let the algorithm do the model selection for us? I will use **Stepwise Algorithm** to do that job. First, I will create two models as the starting point for the algorithm. 

+ null model means no variable is in the model.

+ full model means the model contains all variables and possible interaction terms. 

```{r}
# model with nothing
nullmod <- glm(Survived ~ 1, family = "binomial", data = titanic_train)

# model with all possible interaction term
fullmod <- glm(Survived ~ Pclass*Age*Sex, family = "binomial", data = titanic_train)

summary(fullmod)
```

The are two ways for the **Stepwise Algorithm** to work, one is backward selection and the other is forward selection. Both will give you the same result though. Let me try backward selection first.

```{r}
# backward Stepwise algorithm AIC
bwd <- step(fullmod, direction = "backward")
# show the model backward chose
summary(bwd)
```

Then, let's try forward selection. 

```{r}
# forward stepwise algorithm
fwd <- step(nullmod, scope = list(upper = fullmod), direction = "forward")
# show the model forward chose
summary(fwd)
```

The most convenient way is to use the Stepwise regression. 

```{r, echo = F}
both <- step(nullmod, scope = list(upper = fullmod), direction = "both")
summary(both)
```

Let's visualize the effects of the variables then.
```{r}
plot(effects::allEffects(both))
```


## Build a regularized logistic regression
We use the `glmnet` library to build the regularized logistic regression (elastic nets) and tune the hyperparameter using `caret`. As we want to predict whether a passenger survived (1 = surivied, 0 = didn't) using predictors (age, sex, class). What we will do next:

- Train a model using training set

- Predict survival in test set

```{r, warning=F}
library(glmnet)
library(caret)
reg_logistic <- caret::train(
  Survived ~ Pclass*Age*Sex,
  data = titanic_train, 
  preProc = c("center", "scale", "zv"),
  method = "glmnet",
  family = "binomial",
  trControl = trainControl(
    method = "cv", number = 10
  ),
  tuneLength = 7
)

```

- `preProcess` argument specifies how you preprocess the data before piping it into the model. Use `?preProcess` see pre-processing options of training data. 

- Use `?trainControl` see training options. We use 10-fold cross validation.

- `tuneLength` specifies the regularization parameter `alpha`

Let's then take a look at the training model and see the hypeparameter for the best tune model generated from 10-fold cross validation. 
```{r}
summary(reg_logistic)
reg_logistic$bestTune
```
We can also view the variable importance using `vip` function. `Pclass3` is the most important among all the variables.
```{r}
# plotting variable importance
vip::vip(reg_logistic)
```
To evaluate the model performance, we use confusion matrix because we are dealing with a classification problem.

- The upper left and bottom right cells show the number of correct classifications.

- The upper right and bottom left cells show the number of wrong classifications.

- Accuracy means how many classifications are correctly predicted in total and is calculated using (TP + TN)/(TP+FP+FN+TN)

  - TP(true positive): upper left cell
  
  - TN(true negative): bottom right cell 
  
  - FP(false positive): upper right cell
  
  - FN(false negative): bottom left cell

- Specificity means the proportion of 0s that are correctly classified and is calculated using TN/(TN+FP)

- Sensitivity means the proportion of actual 1s that are classified correctly and is calculated using TP/(TP+FN)

```{r}
# find average bootstrap accuracy
confusionMatrix(reg_logistic)
```

As we can see, the accuracy is *0.8053*. Accuracy estimates how often I classify right or the percentage of times that my model predicts correctly. I estimate that my regularized logistic regression predicts correctly 80% of the times. 

Let's use the trained model to predict on the test set and see the predicted result. The output table shows the probability of each possible result (1 = Survivied, 0 = didn't).  
```{r}
titanic_test <- titanic_test %>% mutate(
  preds_reg = predict(reg_logistic, newdata = titanic_test, type = "prob"),
)
head(titanic_test$preds_reg)
```

We can also visualize the parameter tuning for regularized regression.
```{r,warning=F}
ggplot(reg_logistic)
```













